{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d66510-ed08-4566-8550-959c5257e4a0",
   "metadata": {},
   "source": [
    "# Testing Data Loading from SSD\n",
    "I'm specifically testing 1) whether loading data from the SSD possible, and 2) how fast it is, especially compared with loading data from my local computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a1ccfd-b6eb-4c42-b616-e10902bf2997",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e227afe1-8812-47a4-9362-961bab2a6275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.plotting import heatmap\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import pyarrow as pa\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tulipy as ti\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import HyperParameters\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a41552a-46a5-44de-bc25-fc5a38dc83cd",
   "metadata": {},
   "source": [
    "## Trying to get data from SSD\n",
    "We have our filtered PM 1-min data stored in an SSD, and we're now going to try accessing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e799aa-36d4-492b-b250-c8029d65af8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/danielwang/Desktop/Work stuff/Coding Stuff/Day Trading Stuff/Trading-Strategies-With-ML/Archive'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the current directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f82bb222-4158-4d10-9fd7-341e74b6ac23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/T7/filtered-parquet-PM'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the path to the SSD\n",
    "ssd_name = 'T7'\n",
    "ssd_path = '/Volumes/' + ssd_name + '/'\n",
    "filtered_parquet_PM_path = ssd_path + 'filtered-parquet-PM/'\n",
    "filtered_parquet_PM_HL_path = ssd_path + 'filtered-parquet-PM-HL/'\n",
    "\n",
    "# Changing the directory to the filtered-parquet-PM folders\n",
    "os.chdir(filtered_parquet_PM_path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a0399cfe-ff4d-4442-a0c5-5e7320961635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2073"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(filtered_parquet_PM_path)\n",
    "len(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "94a65d42-4129-4375-bf4d-03b1bd74409b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2073"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting how many folders with tickers there are\n",
    "len(os.listdir(filtered_parquet_PM_HL_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "66cf8a94-165c-4dfa-a4af-2879a589b4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2883"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the data with MSFT\n",
    "MSFT_parq_folder = 'MSFT_1min_parquet'\n",
    "len(os.listdir(MSFT_parq_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c2a92e-bea0-474c-bf50-9555a49f4d40",
   "metadata": {},
   "source": [
    "These numbers match those from the Google Colab testing with the Google Drive folders, which means we have downloaded the data correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8ac4cd-7f41-4cd7-ab56-a3992e0b7504",
   "metadata": {},
   "source": [
    "## Dask testing with data from SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7dbfbf8e-73a0-47cf-a738-c26d5f81c250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the data with NTLA\n",
    "NTLA_parq_folder = 'NTLA_1min_parquet'\n",
    "len(os.listdir(NTLA_parq_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ce587102-745f-4734-8193-1003680eb53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=516</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-02 04:03:00</th>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 00:00:00</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-18 00:00:00</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-18 15:59:00</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: repartition-merge, 8 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                        Open     High      Low    Close   Volume  Ticker\n",
       "npartitions=516                                                         \n",
       "2017-11-02 04:03:00  float64  float64  float64  float64  float64  string\n",
       "2017-11-05 00:00:00      ...      ...      ...      ...      ...     ...\n",
       "...                      ...      ...      ...      ...      ...     ...\n",
       "2024-11-18 00:00:00      ...      ...      ...      ...      ...     ...\n",
       "2024-11-18 15:59:00      ...      ...      ...      ...      ...     ...\n",
       "Dask Name: repartition-merge, 8 graph layers"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing with dask NTLA filtered PM parquet data\n",
    "df_NTLA = dd.read_parquet(NTLA_parq_folder)\n",
    "\n",
    "# This part is optional (if you want to choose a specific datetime range)\n",
    "# ^Note, the time range you chose MUST have data in it for this to work\n",
    "# df_NTLA = df_NTLA.loc['2024-07-15 00:00':'2024-07-30 00:00']\n",
    "# df_NTLA = df_NTLA.loc['2024-12-01 00:00':'2024-12-30 00:00']\n",
    "\n",
    "# Need to do the following to repartition properly\n",
    "df_NTLA = df_NTLA.reset_index()\n",
    "df_NTLA['timestamp'] = df_NTLA['timestamp'].dt.floor('s')\n",
    "df_NTLA = df_NTLA.set_index('timestamp')\n",
    "\n",
    "# # Repartioning for 5 days because data is pretty sparse\n",
    "df_NTLA = df_NTLA.repartition(freq='5D')\n",
    "\n",
    "df_NTLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "41ef9933-d7c8-45ef-a393-063781d2b1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-28 04:06:00</th>\n",
       "      <td>130.00</td>\n",
       "      <td>130.0000</td>\n",
       "      <td>130.00</td>\n",
       "      <td>130.0000</td>\n",
       "      <td>295.0</td>\n",
       "      <td>NTLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-28 04:07:00</th>\n",
       "      <td>125.00</td>\n",
       "      <td>125.0000</td>\n",
       "      <td>123.99</td>\n",
       "      <td>123.9900</td>\n",
       "      <td>238.0</td>\n",
       "      <td>NTLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-28 04:08:00</th>\n",
       "      <td>123.99</td>\n",
       "      <td>123.9900</td>\n",
       "      <td>123.99</td>\n",
       "      <td>123.9900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NTLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-28 04:09:00</th>\n",
       "      <td>121.99</td>\n",
       "      <td>122.0000</td>\n",
       "      <td>121.99</td>\n",
       "      <td>122.0000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>NTLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-28 04:10:00</th>\n",
       "      <td>122.00</td>\n",
       "      <td>122.0000</td>\n",
       "      <td>122.00</td>\n",
       "      <td>122.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NTLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-30 15:55:00</th>\n",
       "      <td>163.65</td>\n",
       "      <td>163.9399</td>\n",
       "      <td>162.83</td>\n",
       "      <td>163.0200</td>\n",
       "      <td>37309.0</td>\n",
       "      <td>NTLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-30 15:56:00</th>\n",
       "      <td>163.21</td>\n",
       "      <td>163.4000</td>\n",
       "      <td>161.58</td>\n",
       "      <td>161.9499</td>\n",
       "      <td>58386.0</td>\n",
       "      <td>NTLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-30 15:57:00</th>\n",
       "      <td>161.91</td>\n",
       "      <td>162.2800</td>\n",
       "      <td>161.56</td>\n",
       "      <td>162.0700</td>\n",
       "      <td>80103.0</td>\n",
       "      <td>NTLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-30 15:58:00</th>\n",
       "      <td>162.11</td>\n",
       "      <td>162.5900</td>\n",
       "      <td>161.99</td>\n",
       "      <td>162.4199</td>\n",
       "      <td>63782.0</td>\n",
       "      <td>NTLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-30 15:59:00</th>\n",
       "      <td>162.42</td>\n",
       "      <td>162.5100</td>\n",
       "      <td>161.63</td>\n",
       "      <td>161.8199</td>\n",
       "      <td>123506.0</td>\n",
       "      <td>NTLA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1419 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Open      High     Low     Close    Volume Ticker\n",
       "timestamp                                                               \n",
       "2021-06-28 04:06:00  130.00  130.0000  130.00  130.0000     295.0   NTLA\n",
       "2021-06-28 04:07:00  125.00  125.0000  123.99  123.9900     238.0   NTLA\n",
       "2021-06-28 04:08:00  123.99  123.9900  123.99  123.9900       0.0   NTLA\n",
       "2021-06-28 04:09:00  121.99  122.0000  121.99  122.0000     200.0   NTLA\n",
       "2021-06-28 04:10:00  122.00  122.0000  122.00  122.0000       0.0   NTLA\n",
       "...                     ...       ...     ...       ...       ...    ...\n",
       "2021-06-30 15:55:00  163.65  163.9399  162.83  163.0200   37309.0   NTLA\n",
       "2021-06-30 15:56:00  163.21  163.4000  161.58  161.9499   58386.0   NTLA\n",
       "2021-06-30 15:57:00  161.91  162.2800  161.56  162.0700   80103.0   NTLA\n",
       "2021-06-30 15:58:00  162.11  162.5900  161.99  162.4199   63782.0   NTLA\n",
       "2021-06-30 15:59:00  162.42  162.5100  161.63  161.8199  123506.0   NTLA\n",
       "\n",
       "[1419 rows x 6 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_NTLA.partitions[250:300].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8c19d2d5-d765-424c-81dc-3fd81e1fc0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-11-18    720\n",
       "2020-06-03    718\n",
       "2017-11-02    717\n",
       "2021-06-28    714\n",
       "2020-12-02    710\n",
       "2021-06-30    705\n",
       "2022-12-01    623\n",
       "2024-10-24    612\n",
       "2022-08-03    563\n",
       "2023-11-01    540\n",
       "2022-09-16    536\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_NTLA = pd.read_parquet(NTLA_parq_folder)\n",
    "pd.DataFrame(df_NTLA.index.date).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de848a2-3946-4822-9112-2568e8dc8d4a",
   "metadata": {},
   "source": [
    "It appears that dask does work as well, so we should be good there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e4a783-6d37-48a9-9bdb-b9a2a5587e0d",
   "metadata": {},
   "source": [
    "## Speed test with loading data from local file vs loading data from SSD\n",
    "I'm now going to take a folder that I've tested before with the exact same 1_min parquet data and compare how fast it is to load that data vs loading data from the SSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "82efda8d-0312-4063-80f3-d95db6f30a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting paths for AAOI_1min_parquet folder\n",
    "orig_AAOI_path = '/Users/danielwang/Desktop/Work stuff/Coding Stuff/Day Trading Group Stuff/AAOI_1min_parquet'\n",
    "ssd_AAOI_path = '/Volumes/T7/filtered-parquet-PM/AAOI_1min_parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c4c73988-2889-45fb-bf4f-81d95c6d4329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in the original AAOI folder path: 27\n",
      "Number of files in the SSD AAOI folder path: 27\n"
     ]
    }
   ],
   "source": [
    "# Confirming the number of files is the same for both\n",
    "print(\"Number of files in the original AAOI folder path:\", len(os.listdir(orig_AAOI_path)))\n",
    "print(\"Number of files in the SSD AAOI folder path:\", len(os.listdir(ssd_AAOI_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3edb6004-dfdb-4944-bd55-7aac26875482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12 ms ± 107 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dd.read_parquet(orig_AAOI_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "cff10e15-a75b-42c9-916e-a56ece56fbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.39 ms ± 761 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dd.read_parquet(ssd_AAOI_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72b4500-894d-4c34-b672-449e321a229b",
   "metadata": {},
   "source": [
    "We now see that it's about twice as slow to load data from the SSD compared to the original folder. However, this is still significantly faster than loading data from the Google Drive, and data loading is typically not the bottle neck anyway, so we should be alright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edabcdb5-6b3a-4f7a-8e0e-88c7559e9d18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
