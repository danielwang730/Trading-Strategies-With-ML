{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d66510-ed08-4566-8550-959c5257e4a0",
   "metadata": {},
   "source": [
    "# Testing Data Loading from SSD\n",
    "I'm specifically testing 1) whether it's possible, and 2) how fast it is, especially compared with loading data from my local computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a1ccfd-b6eb-4c42-b616-e10902bf2997",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e227afe1-8812-47a4-9362-961bab2a6275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.plotting import heatmap\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "\n",
    "import tulipy as ti\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import HyperParameters\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a41552a-46a5-44de-bc25-fc5a38dc83cd",
   "metadata": {},
   "source": [
    "## Trying to get data from SSD\n",
    "We have our filtered PM 1-min data stored in an SSD, and we're now going to try accessing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e799aa-36d4-492b-b250-c8029d65af8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/danielwang/Desktop/Work stuff/Coding Stuff/Day Trading Stuff/Trading-Strategies-With-ML/Archive'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the current directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f82bb222-4158-4d10-9fd7-341e74b6ac23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/T7/filtered-parquet-PM'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the path to the SSD\n",
    "ssd_name = 'T7'\n",
    "ssd_path = '/Volumes/' + ssd_name + '/'\n",
    "filtered_parquet_PM_path = ssd_path + 'filtered-parquet-PM/'\n",
    "filtered_parquet_PM_HL_path = ssd_path + 'filtered-parquet-PM-HL/'\n",
    "\n",
    "# Changing the directory to the filtered-parquet-PM folders\n",
    "os.chdir(filtered_parquet_PM_path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dbfbf8e-73a0-47cf-a738-c26d5f81c250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the data with NYT\n",
    "NYT_parq_folder = 'NYT_1min_parquet'\n",
    "len(os.listdir(NYT_parq_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce587102-745f-4734-8193-1003680eb53b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: Error creating dataset. Could not read schema from '/Volumes/T7/filtered-parquet-PM/MDT_1min_parquet/._2005-11-14_MDT_1min.parquet'. Is this a 'parquet' file?: Could not open Parquet input source '/Volumes/T7/filtered-parquet-PM/MDT_1min_parquet/._2005-11-14_MDT_1min.parquet': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/dask/backends.py:136\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/dask/dataframe/io/parquet/core.py:538\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, columns, filters, categories, index, storage_options, engine, use_nullable_dtypes, dtype_backend, calculate_divisions, ignore_metadata_file, metadata_task_size, split_row_groups, blocksize, aggregate_files, parquet_file_extension, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m     blocksize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m read_metadata_result \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mread_metadata(\n\u001b[1;32m    539\u001b[0m     fs,\n\u001b[1;32m    540\u001b[0m     paths,\n\u001b[1;32m    541\u001b[0m     categories\u001b[38;5;241m=\u001b[39mcategories,\n\u001b[1;32m    542\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m    543\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[1;32m    544\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    545\u001b[0m     gather_statistics\u001b[38;5;241m=\u001b[39mcalculate_divisions,\n\u001b[1;32m    546\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[1;32m    547\u001b[0m     split_row_groups\u001b[38;5;241m=\u001b[39msplit_row_groups,\n\u001b[1;32m    548\u001b[0m     blocksize\u001b[38;5;241m=\u001b[39mblocksize,\n\u001b[1;32m    549\u001b[0m     aggregate_files\u001b[38;5;241m=\u001b[39maggregate_files,\n\u001b[1;32m    550\u001b[0m     ignore_metadata_file\u001b[38;5;241m=\u001b[39mignore_metadata_file,\n\u001b[1;32m    551\u001b[0m     metadata_task_size\u001b[38;5;241m=\u001b[39mmetadata_task_size,\n\u001b[1;32m    552\u001b[0m     parquet_file_extension\u001b[38;5;241m=\u001b[39mparquet_file_extension,\n\u001b[1;32m    553\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset_options,\n\u001b[1;32m    554\u001b[0m     read\u001b[38;5;241m=\u001b[39mread_options,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_options,\n\u001b[1;32m    556\u001b[0m )\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# In the future, we may want to give the engine the\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# option to return a dedicated element for `common_kwargs`.\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# However, to avoid breaking the API, we just embed this\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# data in the first element of `parts` for now.\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# The logic below is inteded to handle backward and forward\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# compatibility with a user-defined engine.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/dask/dataframe/io/parquet/arrow.py:536\u001b[0m, in \u001b[0;36mArrowDatasetEngine.read_metadata\u001b[0;34m(cls, fs, paths, categories, index, use_nullable_dtypes, dtype_backend, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Stage 1: Collect general dataset information\u001b[39;00m\n\u001b[0;32m--> 536\u001b[0m dataset_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_collect_dataset_info(\n\u001b[1;32m    537\u001b[0m     paths,\n\u001b[1;32m    538\u001b[0m     fs,\n\u001b[1;32m    539\u001b[0m     categories,\n\u001b[1;32m    540\u001b[0m     index,\n\u001b[1;32m    541\u001b[0m     gather_statistics,\n\u001b[1;32m    542\u001b[0m     filters,\n\u001b[1;32m    543\u001b[0m     split_row_groups,\n\u001b[1;32m    544\u001b[0m     blocksize,\n\u001b[1;32m    545\u001b[0m     aggregate_files,\n\u001b[1;32m    546\u001b[0m     ignore_metadata_file,\n\u001b[1;32m    547\u001b[0m     metadata_task_size,\n\u001b[1;32m    548\u001b[0m     parquet_file_extension,\n\u001b[1;32m    549\u001b[0m     kwargs,\n\u001b[1;32m    550\u001b[0m )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# Stage 2: Generate output `meta`\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/dask/dataframe/io/parquet/arrow.py:1051\u001b[0m, in \u001b[0;36mArrowDatasetEngine._collect_dataset_info\u001b[0;34m(cls, paths, fs, categories, index, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, kwargs)\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1051\u001b[0m     ds \u001b[38;5;241m=\u001b[39m pa_ds\u001b[38;5;241m.\u001b[39mdataset(\n\u001b[1;32m   1052\u001b[0m         paths,\n\u001b[1;32m   1053\u001b[0m         filesystem\u001b[38;5;241m=\u001b[39m_wrapped_fs(fs),\n\u001b[1;32m   1054\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_processed_dataset_kwargs,\n\u001b[1;32m   1055\u001b[0m     )\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Get file_frag sample and extract physical_schema\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/dataset.py:785\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n\u001b[0;32m--> 785\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _filesystem_dataset(source, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(elem, Dataset) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/dataset.py:475\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    473\u001b[0m factory \u001b[38;5;241m=\u001b[39m FileSystemDatasetFactory(fs, paths_or_selector, \u001b[38;5;28mformat\u001b[39m, options)\n\u001b[0;32m--> 475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m factory\u001b[38;5;241m.\u001b[39mfinish(schema)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/_dataset.pyx:3016\u001b[0m, in \u001b[0;36mpyarrow._dataset.DatasetFactory.finish\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/error.pxi:154\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Error creating dataset. Could not read schema from '/Volumes/T7/filtered-parquet-PM/MDT_1min_parquet/._2005-11-14_MDT_1min.parquet'. Is this a 'parquet' file?: Could not open Parquet input source '/Volumes/T7/filtered-parquet-PM/MDT_1min_parquet/._2005-11-14_MDT_1min.parquet': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Testing with dask NYT filtered PM parquet data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_NYT \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mread_parquet(AAOI_parq_folder)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# This part is optional (if you want to choose a specific datetime range)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# ^Note, the time range you chose MUST have data in it for this to work\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# df_NYT = df_NYT.loc['2024-07-15 00:00':'2024-07-30 00:00']\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# # Repartioning for 5 days because data is pretty sparse\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# df_NYT = df_NYT.repartition(freq='5D')\u001b[39;00m\n\u001b[1;32m     18\u001b[0m df_NYT\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/dask/backends.py:138\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname(func)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod registered to the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m backend.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: Error creating dataset. Could not read schema from '/Volumes/T7/filtered-parquet-PM/MDT_1min_parquet/._2005-11-14_MDT_1min.parquet'. Is this a 'parquet' file?: Could not open Parquet input source '/Volumes/T7/filtered-parquet-PM/MDT_1min_parquet/._2005-11-14_MDT_1min.parquet': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file."
     ]
    }
   ],
   "source": [
    "# Testing with dask NYT filtered PM parquet data\n",
    "df_NYT = dd.read_parquet(AAOI_parq_folder)\n",
    "\n",
    "\n",
    "# This part is optional (if you want to choose a specific datetime range)\n",
    "# ^Note, the time range you chose MUST have data in it for this to work\n",
    "# df_NYT = df_NYT.loc['2024-07-15 00:00':'2024-07-30 00:00']\n",
    "# df_NYT = df_NYT.loc['2024-12-01 00:00':'2024-12-30 00:00']\n",
    "\n",
    "# Need to do the following to repartition properly\n",
    "# df_NYT = df_NYT.reset_index()\n",
    "# df_NYT['timestamp'] = df_NYT['timestamp'].dt.floor('s')\n",
    "# df_NYT = df_NYT.set_index('timestamp')\n",
    "\n",
    "# # Repartioning for 5 days because data is pretty sparse\n",
    "# df_NYT = df_NYT.repartition(freq='5D')\n",
    "\n",
    "df_NYT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c19d2d5-d765-424c-81dc-3fd81e1fc0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-02-20    720\n",
       "2022-11-22    718\n",
       "2023-04-24    717\n",
       "2015-02-17    716\n",
       "2014-06-16    711\n",
       "             ... \n",
       "2008-01-09    480\n",
       "2007-03-05    480\n",
       "2007-08-01    480\n",
       "2007-11-06    480\n",
       "2007-10-15    480\n",
       "Name: count, Length: 62, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_NYT = pd.read_parquet(AAOI_parq_folder)\n",
    "pd.DataFrame(df_NYT.index.date).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f43a0a-3329-4ad8-a521-71fd514ac725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e1d152-c1b8-476d-959e-61ace025be38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a761c6d3-71bf-4015-b7bd-6cb73f58118e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
