{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccd860b8-e487-4864-9a65-2422276a1912",
   "metadata": {},
   "source": [
    "# Data Cleaning ALL 1 Minute Files From FirstRate Data\n",
    "Previously, we had a cleaning process where Hayes started with the entire set of FRD csv files, converted them into parquet files, uploaded them onto Google Drive, and then filtered them down based on a certain set of criteria. I then continued filtering that down even further on Google Drive (through Google Colab), and I ended up with only the parquet day files that had 1) data starting from 8:00am at the latest, and 2) a PM volume of at least 100,000 shares traded. This limited the number of stocks significantly, and we were planning on testing that with the BHOD python backtester I developed.\n",
    "\n",
    "However, we switched gears and started focusing on 1) forward testing through a TWS API bot, and 2) developing ML-based strategies. As of April 2025, Hayes has continued working on the bot, and while I spent the last month working on my own bot as well, I'm dedicating these notebooks to testing ML strategies that do not take in our pre-conceived notions of a \"Stock in Play\" or what might be considered a good entry or exit. Therefore, I'm going to re-download and re-clean all of the files from FirstRate Data, just so we have as much data as possible, and then I'll likely limit the amount of data that we use to a certain subset. More specifically, the plan is to only use data after the market returned to normal after the COVID pandemic (i.e., starting ~October 2020) and build our models based off of future time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d24f9-8ea6-490a-884d-88f30b14140f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c36e14a5-dfea-445d-b5f0-29a177670f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.plotting import heatmap\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import pyarrow as pa\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tulipy as ti\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import HyperParameters\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7c6b2-9df9-4292-9b9e-40533c1066bd",
   "metadata": {},
   "source": [
    "# Converting all FRD csv files to parquet files\n",
    "I've downloaded all the FRD 1-min data onto the SSD, and now I'll begin process of creating a new folder and converting every single one of those files into parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e36cdc-ce14-452a-ac1e-6fb5b5ee0544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edffbffd-434a-47d7-a1ac-2058ef9ce87e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97d28a-5db5-426d-bda2-a4805f4e57f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b622c8-919a-4927-aca6-6efbb597559d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
